% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/varImpact.R
\encoding{utf8}
\name{varImpact}
\alias{varImpact}
\title{Variable importance estimation using causal inference (TMLE)}
\usage{
varImpact(Y, data, V = 2, Q.library = c("SL.glmnet", "SL.mean"),
  g.library = c("SL.stepAIC"), family = "binomial", adjust_cutoff = 10,
  minYs = 15, minCell = 0, ncov = 10, corthres = 0.8, impute = "knn",
  miss.cut = 0.5, verbose = F, parallel = T, digits = 4)
}
\arguments{
\item{Y}{outcome of interest (numeric vector)}

\item{data}{data frame of predictor variables of interest for
which function returns VIM's. (possibly a matrix?)}

\item{V}{Number of cross-validation folds.}

\item{Q.library}{library used by SuperLearner for model of outcome
versus predictors}

\item{g.library}{library used by SuperLearner for model of
predictor variable of interest versus other predictors}

\item{family}{family ('binomial' or 'gaussian')}

\item{adjust_cutoff}{Maximum number of adjustment variables during TMLE. If more
than this cutoff varImpact will attempt to reduce the dimensions to that
number.}

\item{minYs}{mininum # of obs with event  - if it is < minYs, skip VIM}

\item{minCell}{is the cut-off for including a category of
A in analysis, and  presents the minumum of cells in a 2x2 table of the indicator of
that level versus outcome, separately by training and validation
sample}

\item{ncov}{minimum number of covariates to include as adjustment variables (must
be less than # of basis functions of adjustment matrix)}

\item{corthres}{cut-off correlation with explanatory
variable for inclusion of an adjustment variables}

\item{impute}{Type of missing value imputation to conduct. One of: "zero",
"median", "knn" (default).}

\item{miss.cut}{eliminates explanatory (X) variables with proportion
of missing obs > cut.off}

\item{verbose}{Boolean - if TRUE the method will display more detailed output.}

\item{parallel}{Use parallel processing if a backend is registered; enabled by default.}

\item{digits}{Number of digits to round the value labels.}
}
\value{
Results object.
}
\description{
\code{varImpact} returns variable importance statistics ordered
by statistical significance using a combination of data-adaptive target parameter
}
\details{
The function performs the following functions.
 \enumerate{
 \item Drops variables missing > miss.cut of time (tuneable).
 \item Separate out covariates into factors and continuous (ordered).
 \item Drops variables for which their distribution is uneven  - e.g., all 1 value (tuneable)
 separately for factors and numeric variables (ADD MORE DETAIL HERE)
 \item Changes all factors to remove spaces (used for naming dummies later)
 \item Removes spaces from variable names.
 \item Makes dummy variable basis for factors, including naming dummies
 to be traceable to original factor variables later.
 \item Makes new ordered variable of integers mapped to intervals defined by deciles for the ordered numeric variables (automatically makes)
 fewer categories if original variable has < 10 values.
 \item Creates associated list of number of unique values and the list of them
 for each variable for use in variable importance part.
 \item Makes missing covariate basis for both factors and ordered variables
 \item For each variable, after assigning it as A, uses
 optimal histogram function to combine values using the
 distribution of A | Y=1 to avoid very small cell sizes in
 distribution of Y vs. A (tuneable) (ADD DETAIL)
 \item Uses HOPACH* to cluster variables associated confounder/missingness basis for W,
 that uses specified minimum number of adjustment variables.
 \item Finds min and max estimate of E(Ya) w.r.t. a. after looping through
 all values of A* (after processed by histogram)
 \item Returns estimate of E(Ya(max)-Ya(min)) with SE
 \item Things to do include implementing CV-TMLE and allow reporting of results
 that randomly do not have estimates for some of validation samples.
}
*HOPACH is "Hierarchical Ordered Partitioning and Collapsing Hybrid"
}
\section{Authors}{

Alan E. Hubbard and Chris J. Kennedy, University of California, Berkeley
}

\section{References}{

Benjamini, Y., & Hochberg, Y. (1995). \emph{Controlling the false discovery
rate: a practical and powerful approach to multiple testing}. Journal of the
royal statistical society. Series B (Methodological), 289-300.

Gruber, S., & van der Laan, M. J. (2012). \emph{tmle: An R Package for
Targeted Maximum Likelihood Estimation}. Journal of Statistical Software,
51(i13).

Hubbard, A. E., & van der Laan, M. J. (2016). \emph{Mining with inference:
data-adaptive target parameters (pp. 439-452)}. In P. BÃ¼hlmann et al. (Ed.),
\emph{Handbook of Big Data}. CRC Press, Taylor & Francis Group, LLC: Boca
Raton, FL.

van der Laan, M. J., & Pollard, K. S. (2003). \emph{A new algorithm for
hybrid hierarchical clustering with visualization and the bootstrap}. Journal
of Statistical Planning and Inference, 117(2), 275-303.

van der Laan, M. J., Polley, E. C., & Hubbard, A. E. (2007). \emph{Super
learner}. Statistical applications in genetics and molecular biology, 6(1).

van der Laan, M. J., & Rose, S. (2011). \emph{Targeted learning: causal
inference for observational and experimental data}. Springer Science &
Business Media.
}
\examples{
####################################
# Create test dataset.
set.seed(1)
N <- 200
num_normal <- 7
X <- as.data.frame(matrix(rnorm(N * num_normal), N, num_normal))
Y <- rbinom(N, 1, plogis(.2*X[, 1] + .1*X[, 2] - .2*X[, 3] + .1*X[, 3]*X[, 4] - .2*abs(X[, 4])))
# Add some missing data to X so we can test imputation.
for (i in 1:10) X[sample(nrow(X), 1), sample(ncol(X), 1)] <- NA

####################################
# Basic example
vim <- varImpact(Y = Y, data = X)
vim
vim$results_all
exportLatex(vim)

# Impute by median rather than knn.
vim <- varImpact(Y = Y, data = X, impute = "median")

####################################
# doMC parallel (multicore) example.
\dontrun{
library(doMC)
registerDoMC()
# Use L'Ecuyer for multicore seeds; see ?set.seed for details.
set.seed(23432, "L'Ecuyer-CMRG")
vim <- varImpact(Y = Y, data = X)
}

####################################
# doSNOW parallel example.
\dontrun{
library(doSNOW)
library(RhpcBLASctl)
# Detect the number of physical cores on this computer using RhpcBLASctl.
cluster <- makeCluster(get_num_cores())
registerDoSNOW(cluster)
vim <- varImpact(Y = Y, data = X)
stopCluster(cluster)
}

####################################
# mlbench BreastCancer example.
data(BreastCancer, package="mlbench")
data <- BreastCancer
# Create a numeric outcome variable.
data$Y <- as.numeric(data$Class == "malignant")
# Use multicore parallelization to speed up processing.
\dontrun{
doMC::registerDoMC()
}
vim <- varImpact(Y = data$Y, data = subset(data, select=-c(Y, Class, Id)))

}
\seealso{
\code{\link[varImpact]{exportLatex}}, \code{\link[varImpact]{print.varImpact}} method
}

